{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "In the rapidly evolving food industry, the ability to accurately classify food items through images is becoming increasingly critical. As consumers demand more personalized and efficient services, businesses are seeking innovative solutions to enhance their operational efficiency and customer satisfaction. The integration of AI-powered food image classification can revolutionize various aspects of the food ecosystem and  restaurant operations . By automating food identification, businesses can streamline processes, reduce human error, and provide a more engaging user experience.\n",
    "\n",
    "\n",
    "## Problem statement\n",
    "Accurate food classification remains a challenge in the food industry, affecting restaurants, delivery platforms, and nutrition tracking apps. Manual identification leads to errors in  menu categorization and automated checkouts.\n",
    "\n",
    "`An AI-powered food classification model can automate this process with high accuracy, reducing errors, improving user experience, and enhancing operational efficiency.`\n",
    "\n",
    "## Project Goals\n",
    "### Optimization Strategies\n",
    "Improving accuracy beyond 78% is crucial for reliable food identification. Advanced deep learning techniques, data augmentation, and model optimization can enhance classification performance, reducing errors in real-world applications.\n",
    "\n",
    "### Impact on Automation and Service Efficiency\n",
    "AI-powered food classification can streamline restaurant operations, minimize human error, and improve service efficiency. In food delivery, it optimizes workflow, enhances order accuracy, and reduces delays.\n",
    "\n",
    "\n",
    "### Scalability and Real-World Application\n",
    "For widespread adoption, the model must handle diverse food categories efficiently. Ensuring accuracy in automated checkouts, menu categorization, and large-scale applications is key to real-world feasibility.\n",
    "\n",
    "\n",
    "\n",
    "## Stakeholders\n",
    "##### Restaurants and Food Service Providers\n",
    "They will benefit from improved operational efficiency, reduced errors in order processing, and enhanced customer satisfaction.\n",
    "Food Delivery Platforms\n",
    "\n",
    "-These platforms can leverage accurate food classification to streamline their logistics, improve menu categorization, and enhance user experience.\n",
    "\n",
    "##### Consumers\n",
    "End-users will gain from more accurate  personalized dietary recommendations, and a more seamless food ordering experience.\n",
    "\n",
    "##### Health and Nutrition Apps\n",
    "These applications can utilize the classification model to provide users with better insights into their dietary habits and nutritional intake.\n",
    "\n",
    "##### Data Scientists and AI Developers\n",
    "Professionals in this field will be engaged in developing and refining the classification model, contributing to advancements in AI technology.\n",
    "\n",
    "## Beneficiaries\n",
    "##### Consumers\n",
    "They will experience improved accuracy in food selection\n",
    "\n",
    "##### Restaurants\n",
    "Enhanced operational efficiency and reduced errors will lead to cost savings and improved customer loyalty.\n",
    "\n",
    "##### Food Delivery Services\n",
    "Improved accuracy in food classification will streamline operations, reduce delivery times, and enhance customer satisfaction.\n",
    "\n",
    "##### Health Professionals\n",
    "They can utilize accurate food classification data to provide better dietary advice and support to their clients.\n",
    "\n",
    "##### Technology Providers\n",
    "Companies developing AI solutions will benefit from the demand for advanced food classification technologies, leading to potential partnerships and revenue growth.\n",
    "\n",
    "`By addressing these business questions and engaging the identified stakeholders and beneficiaries, the project can create a significant impact on the food industry, enhancing both operational efficiency and consumer experience.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The Food-101 dataset is a large-scale image dataset containing 101,000 images spanning 101 food categories, with 1,000 images per class. It was introduced in the paper \"Food-101 – Mining Discriminative Components with Random Forests\" by Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Dataset Structure\n",
    "*Training Set*: 75,750 images (750 per class)\n",
    "\n",
    "*Test Set*: 25,250 images (250 per class)\n",
    "\n",
    "*Image Format*: RGB, 512 × 512 pixels\n",
    "\n",
    "*Classes*: 101 different food items, including dishes like pizza, sushi, steak, and ramen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Data Characteristics\n",
    "*Imbalance*: The dataset is evenly distributed across all 101 food categories.\n",
    "\n",
    "*Quality Issues*: The training set contains some noisy labels, making it slightly challenging for model training.\n",
    "\n",
    "*Data Augmentation*: Since the dataset lacks variations in angles, lighting, and occlusions, augmentation techniques like \n",
    "rotation, flipping, and color jittering can improve model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data-Source-tensorflowdatasets(tfds)\n",
    "tfds- is an online source[https://www.tensorflow.org/datasets/catalog/food101?hl=en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available datasets and check if the Food101 dataset is present in the tensorflow dataset\n",
    "dataset_list =tfds.list_builders()\n",
    "print('food101' in dataset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data () Takes a while atleast 10 minutes\n",
    "(train_data,test_data), ds_info =tfds.load(name = 'food101',\n",
    "                                           split = ['train','validation'],\n",
    "                                           shuffle_files=True,\n",
    "                                           as_supervised=True, # data gets returned in tuple format (data,label)\n",
    "                                           with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance of the Dataset\n",
    "The Food-101 dataset is a widely used benchmark for food classification, offering 101,000 images across 101 diverse food categories. It is valuable for applications like restaurant recommendation systems, calorie estimation, and AI-driven dietary monitoring.\n",
    "\n",
    "The dataset supports ;\n",
    "\n",
    "*Fine-Grained Classification: Allows for detailed and accurate classification of similar food items.*\n",
    "\n",
    "*State-of-the-Art Models: Leverages advanced models for robust and efficient performance.*\n",
    "\n",
    "*Global Variety: Ensures models are generalizable across diverse culinary contexts.*\n",
    "\n",
    "*Real-World Noisy Labels: Prepares models to handle imperfections and real-world conditions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Food 101 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By becoming one with the data we aim to get:\n",
    "* `class names` - we're working with 101 different food classes\n",
    "* The shape of our input data (image tensors)\n",
    "* The datatype of our input data\n",
    "* What the labels look like (e.g are they one-hot encoded or are they label encoded)\n",
    "* Do the labels match up with the class names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the class names\n",
    "class_names=ds_info.features['label'].names\n",
    "print('Length:',len(class_names))\n",
    "class_names[:10] # Extract the first 10 names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take on sample of the train data\n",
    "train_one_sample = train_data.take(1)  #(image_tensor,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image,labels in train_one_sample:\n",
    "  print(f\"\"\"\n",
    "  Image shape: {image.shape}\n",
    "  Image datatype: {image.dtype}\n",
    "  Target class from Food101 (tensor form): {labels}\n",
    "  Class names (str form): {class_names[labels.numpy()]}\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the image tensors look like\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the min and max values of image tensor?\n",
    "tf.reduce_min(image),tf.reduce_max(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot an image tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(image)\n",
    "plt.title(class_names[labels.numpy()])\n",
    "plt.axis('off');\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function that plots a given number of  random image from the TFDS Food101 dataset\n",
    "def TFDS_plot(train_data,nrows=2,ncol=5,Class_names =class_names,plot_no =10):\n",
    "  #Loop through the sample and extract the label and image\n",
    "\n",
    "# Plot the data\n",
    "   images = []\n",
    "   labels=[]\n",
    "\n",
    "   for image,label in train_data.take(plot_no):\n",
    "      images.append(image),\n",
    "      labels.append(label)\n",
    "\n",
    "   plt.figure(figsize=(10,8))\n",
    "   for i in range(plot_no):\n",
    "      k = i+ 1\n",
    "      plot_data=plt.subplot(nrows,ncol,k) # has to be adjusted based\n",
    "      plot_data=plt.imshow(images[i])\n",
    "      plot_data=plt.title(Class_names[labels[i].numpy()])\n",
    "      plot_data=plt.axis('off')\n",
    "      plot_data=plt.tight_layout()\n",
    "      i += 1\n",
    "   return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFDS_plot(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Preprocessing Functions for Our Data  \n",
    "\n",
    "Neural networks achieve optimal performance when data is formatted in a specific way (e.g., batched, normalized, etc.). However, raw data—especially from TensorFlow datasets—often requires preprocessing to meet these requirements.  \n",
    "\n",
    "#### Key Characteristics of Our Data:  \n",
    "- Stored in `uint8` format  \n",
    "- Contains images of varying sizes  \n",
    "- Pixel values range from 0 to 255 (not yet normalized)  \n",
    "\n",
    "#### What Our Model Prefers:  \n",
    "- Data in `float32` format (or `float16`/`float32` for mixed precision)  \n",
    "- Uniform image sizes within each batch  \n",
    "- Scaled pixel values (0 to 1) for improved model performance  \n",
    "\n",
    "#### Preprocessing Requirements:  \n",
    "Since we are using an **EfficientNetBX** pretrained model from `tf.keras.applications`, explicit rescaling is unnecessary as these models include built-in rescaling.  \n",
    "\n",
    "Thus, our preprocessing function should:  \n",
    "1. Resize all images to a consistent shape.  \n",
    "2. Convert image tensors from `uint8` to `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for preprocessing images\n",
    "def preprocess_img(image,label,img_shape=224):\n",
    "  \"\"\"\n",
    "  Converts image datatype from `uint8` -> `float 32` and reshapes\n",
    "  the image shape and color channels-|\n",
    "  [img_shape,img_shape,color channel]\n",
    "  \"\"\"\n",
    "  image =tf.image.resize(image,[img_shape,img_shape]) # Reshape target image\n",
    "  # image =image/255. # scale image value (Depends on the model in use)\n",
    "  return tf.cast(image,tf.float32), label #return (float32_image, label) tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch & Prepare datasets\n",
    "\n",
    "We're going to make our data input pipeline run really fast.\n",
    "\n",
    "For more resources on this, I'd highlighly recommend [Pipipeline Introduction:](https://www.tensorflow.org/guide/data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map preprocessing functions to training (and parallelize)\n",
    "train_data =train_data.map(map_func=preprocess_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle train_data and turn it into batches and prefech it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size =32).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "\n",
    "# Map preprocessing function to test data\n",
    "test_data =test_data.map(map_func=preprocess_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data =test_data.batch(32).prefetch(buffer_size =tf.data.AUTOTUNE) # No need to shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup mixed precision training\n",
    "\n",
    "Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the 32-bit types for numeric stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy, for a deeper understanding of mixed precision training, check out the tensorflow guide for [mixed precision:](https://www.tensorflow.org/guide/mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on mixed precision training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16') #Set global data to mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision.global_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range =0.4,horizontal_flip=True,shear_range =0.3)\n",
    "\n",
    "# Load an image\n",
    "img_sample = train_data.take(1)\n",
    "\n",
    "for img,label in img_sample:\n",
    "  img,label\n",
    "\n",
    "\n",
    "#  Add the image to a batch.\n",
    "img = tf.cast(tf.expand_dims(img, 0), tf.float32)\n",
    "# iterator\n",
    "aug_iter = datagen.flow(img, batch_size=1)\n",
    "\n",
    "# generate samples and plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,15))\n",
    "\n",
    "# generate batch of images\n",
    "for i in range(3):\n",
    "\n",
    "\t# convert to unsigned integers\n",
    "\timage = next(aug_iter)[0].astype('uint8')\n",
    "\n",
    "\t# plot image\n",
    "\tax[i].imshow(image)\n",
    "\tax[i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature extraction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a feature extraction model for food classification simplifies complex data, improves model performance, and reduces training time by distilling raw information (like images) into meaningful features. It enables the model to capture critical patterns such as color, texture, and shape, which are essential for distinguishing between different food types. This process not only enhances classification accuracy but also helps handle variations in food images (e.g., lighting or background). Additionally, feature extraction allows for transfer learning, leveraging pre-trained models to accelerate training and optimize performance, ultimately creating a more efficient and robust classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding our best base model to proceed with fine-tuning\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top = False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Creating a Functional API model\n",
    "inputs = layers.Input(shape=input_shape,name = \"input_layer\")\n",
    "# Since the Efficient models have rescaling built-in we will not include a layer for that\n",
    "# x = preprocessing.Rescaling(1/255.)(x)\n",
    "\n",
    "x = base_model(inputs,training=False) # Just to enforce no updating the model weights\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(len(class_names))(x)\n",
    "\n",
    "# To make sure the output tensors are in float 32 for numerical stability\n",
    "outputs = layers.Activation('softmax',dtype=tf.float32,name='Softmax_layer')(x)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'],\n",
    "              optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dtype_pocies attribute of layers in our model\n",
    "for layer in model.layers:\n",
    "  print(layer.name,layer.trainable ,layer.dtype,layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is present\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a base model for food classification with the following configurations:\n",
    "\n",
    "1. 3 epochs of training.\n",
    "2. Use the ModelCheckpoint callback to save the best model weights during training.\n",
    "3. Integrate Weights & Biases (W&B) for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and prepare  wandb metrics\n",
    "import wandb\n",
    "\n",
    "from wandb.integration.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for the weights and biases\n",
    "configs =dict(\n",
    "    batch_size =32,\n",
    "    num_classes =len(class_names),\n",
    "    shuffle_buffer = 1000,\n",
    "    image_size = 224,\n",
    "    image_channels = 3,\n",
    "    earlystopping_patience =3,\n",
    "    learning_rate = 1e-3,\n",
    "    epochs = 3 # to be changed for the different models\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run =wandb.init(\n",
    "    project = 'Food-Image-Classification',\n",
    "    config =configs\n",
    ")\n",
    "\n",
    "# Using the exact replica of the Transfer learning data\n",
    "Big_vision_history =model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch = int((0.5*len(train_data))), # 10% data\n",
    "    epochs = configs['epochs'],\n",
    "    validation_data = test_data.repeat(),\n",
    "    validation_steps= int(0.15*len(test_data)), # 15 % of the data\n",
    "    callbacks =[WandbMetricsLogger(log_freq =10)]\n",
    ")\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the process of transforming images to create new ones, for training machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range =0.2,horizontal_flip=True)\n",
    "\n",
    "# Load an image\n",
    "img_sample = train_data.take(1)\n",
    "\n",
    "for img,label in img_sample:\n",
    "  img,label\n",
    "\n",
    "\n",
    "# iterator\n",
    "aug_iter = datagen.flow(img, batch_size=32)\n",
    "\n",
    "# generate samples and plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,15))\n",
    "\n",
    "# generate batch of images\n",
    "for i in range(3):\n",
    "\n",
    "\t# convert to unsigned integers\n",
    "\timage = next(aug_iter)[0].astype('uint8')\n",
    "\n",
    "\t# plot image\n",
    "\tax[i].imshow(image)\n",
    "\tax[i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Data Augmentation on the data as a layer (has benefits while using GPU)\n",
    "tf.random.set_seed(42)\n",
    "IMG_SIZE = (224,224)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Input(shape = IMG_SIZE +(3,)),\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomZoom(0.2),\n",
    "    # layers.RandomRotation(0.2),\n",
    "    # layers.RandomHeight(0.2),\n",
    "    # layers.RandomWidth(0.2)\n",
    "],name = 'data_augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding our best base model to proceed with fine-tuning\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top = False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Creating a Functional API model\n",
    "inputs = layers.Input(shape=input_shape,name = \"input_layer\")\n",
    "\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "x = base_model(inputs,training=False) # Just to enforce no updating the model weights\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(len(class_names))(x)\n",
    "\n",
    "# To make sure the output tensors are in float 32 for numerical stability\n",
    "outputs = layers.Activation('softmax',dtype=tf.float32,name='Softmax_layer')(x)\n",
    "model2 = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'],\n",
    "              optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the epochs\n",
    "configs['epochs'] = 6\n",
    "\n",
    "run =wandb.init(\n",
    "    project = 'Food-Image-Classification',\n",
    "    config =configs\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Using the exact replica of the Transfer learning data\n",
    "Big_vision_history =model2.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch = int((0.5*len(train_data))), # 50% data\n",
    "    epochs = configs['epochs'],\n",
    "    validation_data = test_data.repeat(),\n",
    "    validation_steps= int(0.15*len(test_data)), # 15 % of the data\n",
    "    callbacks =[WandbMetricsLogger(log_freq =10)]\n",
    ")\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the augmented Images\n",
    "data_augmented=tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomZoom(0.2)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Another_sample=train_data.take(1)\n",
    "\n",
    "for image,label in Another_sample:\n",
    "  image,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image =image[1]\n",
    "# Add the image to a batch.\n",
    "image = tf.cast(tf.expand_dims(new_image, 0), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag = image[0].numpy()\n",
    "print(f\"Data Type: {imag.dtype}, Min: {imag.min()}, Max: {imag.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  augmented_image = data_augmented(image)\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  # Convert to numpy and check the value range\n",
    "  img = augmented_image[0].numpy()  # Convert from Tensor to NumPy array\n",
    "\n",
    "  if img.dtype == 'float32' or img.dtype == 'float64':  # Normalize if necessary\n",
    "        img = img.clip(0, 1)  # Ensure values are within [0,1] if float\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
